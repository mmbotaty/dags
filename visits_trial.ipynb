{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "vists_dags_func\n",
    "\n",
    "\"\"\"\n",
    "folder_path = '/root/airflow_final/visits_folder'\n",
    "\n",
    "#A function that scan tables from excel workbook\n",
    "def extract_visits_data(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all Excel files (.xlsx) in a folder and returns a dictionary of DataFrames, where each key is the filename\n",
    "    (without extension) and the value is the corresponding DataFrame.\n",
    "    \"\"\"\n",
    "    excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsx')]\n",
    "    dataframes = {}\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        dfs = pd.read_excel(file_path, engine='openpyxl', sheet_name=None)      \n",
    "        for sheet_name, df in dfs.items():\n",
    "            dataframes[f'{file[:-5]}_{sheet_name}'] = df\n",
    "    \n",
    "    return dataframes\n",
    "\n",
    "def load_workbooks(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all Excel workbooks (.xlsx) in a folder and returns a dictionary of DataFrames, where each key is the facility\n",
    "    name and the value is a dictionary of DataFrames, where each key is the sheet name (except 'main info') and the value\n",
    "    is the corresponding DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - folder_path: A string with the path of the folder containing the Excel workbooks.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary of DataFrames, where each key is the facility name and the value is a dictionary of DataFrames, where\n",
    "    each key is the sheet name (except 'main info') and the value is the corresponding DataFrame.\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError if the folder_path argument is None or empty.\n",
    "    - Exception if there's an error reading the Excel workbooks or extracting the data.\n",
    "    \n",
    "    The function reads all Excel workbooks (.xlsm) in the folder_path directory and extracts the data from the sheets\n",
    "    (except 'main info') into a dictionary of DataFrames, where each key is the sheet name (except 'main info') and the\n",
    "    value is the corresponding DataFrame. The function also reads the 'main info' sheet in each workbook to get the\n",
    "    facility names, which will be used as the keys in the dictionary of DataFrames. If a facility doesn't have data in\n",
    "    any of the sheets, its corresponding value in the dictionary of DataFrames will be an empty DataFrame. The function\n",
    "    returns the resulting dictionary of DataFrames.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    excel_files = [f for f in os.listdir(folder_path) if f.endswith('.xlsm')]\n",
    "    workbooks = []\n",
    "    for file in excel_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        excel_file = pd.ExcelFile(file_path)\n",
    "        dfs = {}\n",
    "        for sheet_name in excel_file.sheet_names:\n",
    "            if sheet_name != 'Main Info':\n",
    "                df = pd.read_excel(excel_file, sheet_name, index_col='Facility Name')\n",
    "                dfs[sheet_name] = df\n",
    "        main_info_df = pd.read_excel(excel_file, 'Main Info')\n",
    "        workbook_dfs = [main_info_df]\n",
    "        for sheet_name, df in dfs.items():\n",
    "            joined_df = main_info_df.join(df, on='Facility Name', how='left', rsuffix='_'+sheet_name)\n",
    "            workbook_dfs.append(joined_df)\n",
    "        workbook_df = pd.concat(workbook_dfs, axis=1)\n",
    "        workbooks.append(workbook_df)\n",
    "    return workbooks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/root/airflow_env/lib/python3.9/site-packages/openpyxl/worksheet/_reader.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">329</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> UserWarning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Data Validation extension is not supported and will be removed</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/root/airflow_env/lib/python3.9/site-packages/openpyxl/worksheet/\u001b[0m\u001b[1;33m_reader.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m329\u001b[0m\u001b[1;33m UserWarning\u001b[0m\u001b[33m: Data Validation extension is not supported and will be removed\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: Index Facility Name invalid\n"
     ]
    }
   ],
   "source": [
    "# result = load_workbooks(folder_path)\n",
    "# main_info_df = result.workbook_df[0].loc[:, 'Main Info']\n",
    "try:\n",
    "    workbooks = load_workbooks(folder_path)\n",
    "    for workbook in workbooks:\n",
    "        print(workbook)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">/tmp/ipykernel_3342532/3712467480.py:</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">9</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\"> RemovedInAirflow3Warning</span><span style=\"color: #808000; text-decoration-color: #808000\">: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;33m/tmp/ipykernel_3342532/\u001b[0m\u001b[1;33m3712467480.py\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m9\u001b[0m\u001b[1;33m RemovedInAirflow3Warning\u001b[0m\u001b[33m: Param `schedule_interval` is deprecated and will be removed in a future release. Please use `schedule` instead.\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the DAG\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'start_date': datetime(2023, 4, 10)\n",
    "}\n",
    "\n",
    "dag = DAG('visits_dag_func', default_args=default_args, schedule_interval=\"@daily\",catchup=False)\n",
    "\n",
    "\n",
    "# Define the first task\n",
    "\n",
    "extract_data = PythonOperator(\n",
    "    task_id='extract_data',\n",
    "    python_callable= extract_visits_data,\n",
    "    op_kwargs={'folder_path': folder_path},\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "# Define the second task\n",
    "\n",
    "check_for_row_updates = BranchPythonOperator(\n",
    "    task_id='check_for_row_updates',\n",
    "    python_callable=check_for_row_updates,\n",
    "    dag=dag\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_facility_data(dataframe, table_name):\n",
    "    \"\"\"\n",
    "    Updates facility data in a database table based on changes in a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "    - dataframe: A pandas DataFrame with facility data, where the index corresponds to the facility name.\n",
    "    - table_name: A string with the name of the database table to update.\n",
    "    \n",
    "    Returns:\n",
    "    - A tuple with the number of rows updated and the number of rows added to the database table.\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError if the DataFrame or table_name argument is None or empty.\n",
    "    - Exception if there's an error connecting to or updating the database table.\n",
    "    \n",
    "    The function checks if each facility in the DataFrame has updates in the corresponding table in the database,\n",
    "    based on the facility name (which should act as the primary key). If there are updates, the function replaces\n",
    "    the facility data in the table with the data from the DataFrame. If a facility in the DataFrame doesn't exist\n",
    "    in the table, the function adds a new row to the table with the facility data from the DataFrame. The function\n",
    "    returns a tuple with the number of rows updated and the number of rows added to the database table.\n",
    "    \"\"\"\n",
    "\n",
    "    if dataframe is None or dataframe.empty:\n",
    "        raise ValueError('The dataframe argument is empty.')\n",
    "    if table_name is None or table_name == '':\n",
    "        raise ValueError('The table_name argument is empty.')\n",
    "    \n",
    "    # Connect to the database\n",
    "    connection = connect_to_database()\n",
    "    \n",
    "    # Get the column names from the database table\n",
    "    cursor = connection.cursor()\n",
    "    cursor.execute(f'SELECT TOP 0 * FROM {table_name}')\n",
    "    column_names = [column[0] for column in cursor.description]\n",
    "    \n",
    "    # Create a list of tuples with the facility data from the DataFrame\n",
    "    values = []\n",
    "    for index, row in dataframe.iterrows():\n",
    "        values.append(tuple(row[column] for column in column_names))\n",
    "    \n",
    "    # Create a list of column names to use in the SQL query\n",
    "    columns = ', '.join(column_names)\n",
    "    \n",
    "    # Create a list of column names to use in the SQL query\n",
    "    placeholders = ', '.join('?' * len(column_names))\n",
    "    \n",
    "    # Create a list of column names to use in the SQL query\n",
    "    updates = ', '.join(f'{column} = excluded.{column}' for column in column_names)\n",
    "    \n",
    "    # Create the SQL query\n",
    "    sql = f'INSERT INTO {table_name} ({columns}) VALUES ({placeholders}) ON CONFLICT (facility_name) DO UPDATE SET {updates}'\n",
    "    \n",
    "    # Execute the SQL query\n",
    "    cursor.executemany(sql, values)\n",
    "    \n",
    "    # Commit the changes to the database\n",
    "    connection.commit()\n",
    "    \n",
    "    # Return the number of rows updated and the number of rows added\n",
    "    return cursor.rowcount, len(values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airflow_env",
   "language": "python",
   "name": "airflow_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
